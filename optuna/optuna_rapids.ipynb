{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Optuna + RAPIDS\n",
    "\n",
    "Optuna is a lightweight framework for hyperparameter optimization. It provides a code-by-run method which makes it easy to adapt to any already existing code that we have. Just wrapping the objective function with Optuna can help perform a parallel-distributed HPO search over a search space.\n",
    "\n",
    "We'll explore how to use Optuna with RAPIDS and run multi-GPU HPO runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes - \n",
    "\n",
    "1. Using default SVM parameters (\"Rbf\" kernel) with full airline data results in `cudaErrorMemoryAllocation` - out of memory when `fit` is called.\n",
    "2. Even with 1/10th the data linear kernel hangs for a long time (is this a possible bug or expected behavior?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/treelite/gallery/__init__.py:7: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/treelite/gallery/sklearn/__init__.py:9: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cudf\n",
    "import cuml\n",
    "import dask.array as da\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from cuml.metrics import accuracy_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from dask.distributed import Client, wait, performance_report\n",
    "from joblib import parallel_backend\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for timing blocks of code.\n",
    "@contextmanager\n",
    "def timed(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(\"..%-24s:  %8.4f\" % (name, t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.17.0.2:42429</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.17.0.2:8002/status' target='_blank'>http://172.17.0.2:8002/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>49.16 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.17.0.2:42429' processes=2 threads=2, memory=49.16 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(threads_per_worker=1, ip=\"\", dashboard_address=\"8002\")\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_streams = 8 # Performance optimization\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll load the airline data from the path specified by `INPUT_FILE`. The aim of the problem is to predict whether a plane will be delayed or not by the target variable `ArrDelayBinary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 10\n",
    "\n",
    "INPUT_FILE = \"/home/hyperopt/hyperopt/data/air_par.parquet\"\n",
    "df = cudf.read_parquet(INPUT_FILE)\n",
    "X, y = df.drop([\"ArrDelayBinary\"], axis=1), df[\"ArrDelayBinary\"].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "Here, we define `train_and_eval` function which simply fits a RandomForestClassifier (with`max_depth` and `n_estimators`) on the passed `X_param`, `y_param`. This function should look very similar for any ML workflow. We'll use this function within the Optuna `objective` function to show how easily we can fit an existing workflow into the Optuna work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(X_param, y_param, max_depth, n_estimators):\n",
    "\n",
    "    classifier = RandomForestClassifier(max_depth=max_depth,\n",
    "                         n_estimators=n_estimators)\n",
    "\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_param, y_param, random_state=77)\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_valid)\n",
    "    score = accuracy_score(y_valid, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a baseline number, let's see what the default performance of RFC is. Note the defauly values for `max_depth` = 16 and `n_estimators` = 100; we pass these to the `train_and_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with default parameters :  0.837736189365387\n"
     ]
    }
   ],
   "source": [
    "print(\"Score with default parameters : \",train_and_eval(X, y, max_depth=16, n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "The objective function will be the one we optimize in Optuna studys. Objective funciton tries out different values for the parameters that we are tuning and saving the results in `study.trials_dataframes()`. \n",
    "\n",
    "Let's define the objective function for this HPO task by making use of the `train_and_eval()`. You can see that we simply choose a value for the parameters and call the `train_and_eval` method, making Optuna very easy to use in an existing workflow.\n",
    "\n",
    "The objective remains constant over different samplers, which are built-in options in Optuna to enable the selection of different sampling algorithms that optuna provides. Some of the available ones include - GridSampler, RandomSampler, TPESampler, etc. We'll try out different samplers and compare their performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_param, y_param):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 15)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 150, 750)\n",
    "    score = train_and_eval(X_param, y_param, max_depth=max_depth, n_estimators=n_estimators)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Trials and Study\n",
    "\n",
    "Optuna uses [study](https://optuna.readthedocs.io/en/stable/reference/study.html) and [trials](https://optuna.readthedocs.io/en/stable/reference/trial.html) to keep track of the HPO experiments. \n",
    "\n",
    "We'll make use of a helper function `run_study` to help us run one multi-GPU study with a dask backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_study(sampler=optuna.samplers.TPESampler(), study_name=\"Optuna-MultiGPU\", callbacks=None):\n",
    "    \n",
    "    with timed(study_name):\n",
    "        study = optuna.create_study(sampler=sampler,\n",
    "                                    study_name=study_name,\n",
    "                                    storage=\"sqlite:///_\"+study_name+\".db\",\n",
    "                                    direction=\"maximize\",\n",
    "                                    load_if_exists=True)\n",
    "        \n",
    "        with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "            study.optimize(lambda trial: objective(trial, X, y), n_trials=N_TRIALS, n_jobs=n_workers,\n",
    "                          callbacks=callbacks)\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: \", trial.value)\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 01:54:19,555] A new study created with name: Optuna-MultiGPU-TPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..multi-gpu               :  206.6237\n",
      "Number of finished trials:  10\n",
      "Best trial:\n",
      "  Value:  0.8342416286468506\n",
      "  Params: \n",
      "    max_depth: 10\n",
      "    n_estimators: 605\n"
     ]
    }
   ],
   "source": [
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    study_tpe = run_study(optuna.samplers.TPESampler(),study_name=\"Optuna-MultiGPU-TPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 01:58:00,980] Using an existing study with name 'Optuna-TPE-w' instead of creating a new one.\n",
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..no-dask                 :  338.0006\n"
     ]
    }
   ],
   "source": [
    "with timed(\"no-dask\"):\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                                study_name=\"Optuna-TPE-w\",\n",
    "                                storage=\"sqlite:///optuna_simple.db\",\n",
    "                                direction=\"maximize\",\n",
    "                                load_if_exists=True)\n",
    "    with parallel_backend(\"loky\", n_jobs=n_workers):\n",
    "        study.optimize(lambda trial: objective(trial, X, y), n_trials=N_TRIALS, n_jobs=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 02:03:39,133] A new study created with name: Optuna-TPE\n",
      "[I 2020-07-02 02:03:53,735] Finished trial#0 with value: 0.8308449983596802 with parameters: {'max_depth': 12, 'n_estimators': 234}. Best is trial#0 with value: 0.8308449983596802.\n",
      "[I 2020-07-02 02:04:12,032] Finished trial#1 with value: 0.8341159820556641 with parameters: {'max_depth': 10, 'n_estimators': 388}. Best is trial#1 with value: 0.8341159820556641.\n",
      "[I 2020-07-02 02:04:22,824] Finished trial#2 with value: 0.8310717940330505 with parameters: {'max_depth': 11, 'n_estimators': 192}. Best is trial#1 with value: 0.8341159820556641.\n",
      "[I 2020-07-02 02:04:37,358] Finished trial#3 with value: 0.8308327794075012 with parameters: {'max_depth': 12, 'n_estimators': 235}. Best is trial#1 with value: 0.8341159820556641.\n",
      "[I 2020-07-02 02:05:06,128] Finished trial#4 with value: 0.8307821750640869 with parameters: {'max_depth': 13, 'n_estimators': 409}. Best is trial#1 with value: 0.8341159820556641.\n",
      "[I 2020-07-02 02:05:35,388] Finished trial#5 with value: 0.8308926224708557 with parameters: {'max_depth': 12, 'n_estimators': 474}. Best is trial#1 with value: 0.8341159820556641.\n",
      "[I 2020-07-02 02:06:27,684] Finished trial#6 with value: 0.8307737708091736 with parameters: {'max_depth': 13, 'n_estimators': 732}. Best is trial#1 with value: 0.8341159820556641.\n",
      "[I 2020-07-02 02:07:49,206] Finished trial#7 with value: 0.8430714011192322 with parameters: {'max_depth': 15, 'n_estimators': 629}. Best is trial#7 with value: 0.8430714011192322.\n",
      "[I 2020-07-02 02:08:14,282] Finished trial#8 with value: 0.8307592272758484 with parameters: {'max_depth': 13, 'n_estimators': 362}. Best is trial#7 with value: 0.8430714011192322.\n",
      "[I 2020-07-02 02:08:51,326] Finished trial#9 with value: 0.8307896256446838 with parameters: {'max_depth': 14, 'n_estimators': 462}. Best is trial#7 with value: 0.8430714011192322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..no-dask-no-joblib       :  312.3613\n"
     ]
    }
   ],
   "source": [
    "with timed(\"no-dask-no-joblib\"):\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                                study_name=\"Optuna-TPE\",\n",
    "                                storage=\"sqlite:///optuna_no_joblib.db\",\n",
    "                                direction=\"maximize\",\n",
    "                                load_if_exists=True)\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_cmae = run_study(optuna.samplers.CmaEsSampler(), study_name=\"Optuna-MultiGPU-CMAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential calls without Optuna\n",
    "\n",
    "For a comparison let's try sequential calls without Optuna and it's parallel-processing support. We can cleared see that it takes more time to do this. We'll pick the same parameters as Optuna for a fair comparison - these parameters were selected by the sampling algorithm used by Optuna and is available in the `study.trials_dataframe()` for us to pick out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study_tpe.trials_dataframe()\n",
    "params_max_depth, params_n_estimators = df['params_max_depth'], df['params_n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential call function \n",
    "\n",
    "For a cleaner look, let's use a function to perform sequential calls. The function basically sets the parameters to what was passed and trains and evaluates the model and returns the details of the run which can later be used to find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_call(X, y, max_depth, n_estimators):\n",
    "    \n",
    "    score = train_and_eval(X, y, max_depth=max_depth, n_estimators = n_estimators)\n",
    "    \n",
    "    return score, max_depth, n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8308145999908447, 14, 523), (0.830810010433197, 14, 292), (0.8342475891113281, 10, 603), (0.8342695832252502, 10, 605), (0.8308290243148804, 14, 709), (0.830955982208252, 11, 268), (0.8307737708091736, 13, 540), (0.830833375453949, 14, 300), (0.8308601975440979, 12, 543), (0.8307647705078125, 13, 324)]\n",
      "..no-optuna-call          :  244.6982\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "with timed(\"no-optuna-call\"):\n",
    "    with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "        results = Parallel()(delayed(seq_call)(X, y, max_depth=params_max_depth[i],\n",
    "                     n_estimators=params_n_estimators[i]) for i in range(N_TRIALS))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Running this without a dask backend is actually faster - takes about 65 seconds to finish by just making N_TRIALS sequential calls. Dask backend makes most sense when used with multi-GPU estimators as we see later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8307656049728394, 13, 324)\n",
      "..no-optuna-no-dask       :  309.5257\n"
     ]
    }
   ],
   "source": [
    "with timed(\"no-optuna-no-dask\"):\n",
    "    for i in range(N_TRIALS):\n",
    "        results = seq_call(X, y, max_depth=params_max_depth[i],\n",
    "                     n_estimators=params_n_estimators[i])\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow callback\n",
    "\n",
    "Optuna supports the integration of various libraries. One of them is a tracking library MLflow, this is used to keep track of the different Hyperopt runs. We can simply add it by adding a callback to a study as shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_callback(study, trial):\n",
    "    trial_value = trial.value if trial.value is not None else float(\"nan\")\n",
    "    with mlflow.start_run(run_name=study.study_name):\n",
    "        print(trial.params)\n",
    "        mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "        mlflow.log_params(trial.params)\n",
    "        mlflow.log_metrics({\"accuracy\": trial_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = run_study(optuna.samplers.TPESampler(),study_name=\"Optuna-MultiGPU-MLflow\", callbacks=[mlflow_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU estimators\n",
    "\n",
    "We also have estimators that can run on multiple GPUs. `cuml.dask` has a set of multi-GPU estimators that can run incredibly fast. Let's try that out. In order to do this, we need to used `dask_cudf` dataframes and we will redefine the objective function from earlier to do just that. \n",
    "\n",
    "`objective_mg` converts our split data into dask_cudf dataframes and persists them across all available dask workers. By doing this, we can now run the multi-GPU RandomForestClassifier. Notice that we import the `cuml.dask.ensemble.RandomForestClassifier` for this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.ensemble import RandomForestClassifier as dask_RF\n",
    "\n",
    "def objective_mg(trial):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 15)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 150, 750)\n",
    "\n",
    "    import dask_cudf \n",
    "    \n",
    "    classifier = dask_RF(max_depth=max_depth,\n",
    "                         n_estimators=n_estimators)\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "\n",
    "    X_train_dask = dask_cudf.from_cudf(X_train, npartitions=2)\n",
    "    X_valid_dask = dask_cudf.from_cudf(X_valid, npartitions=2)\n",
    "    \n",
    "    y_train_dask = dask_cudf.from_cudf(y_train, npartitions=2)\n",
    "    y_valid_dask = dask_cudf.from_cudf(y_valid, npartitions=2)\n",
    "    \n",
    "    X_train_dask, X_valid_dask, y_train_dask, y_valid_dask = dask_utils.persist_across_workers(c,\n",
    "                                                                                               [X_train_dask, \n",
    "                                                                                                X_valid_dask,\n",
    "                                                                                                y_train_dask,\n",
    "                                                                                                y_valid_dask],\n",
    "                                                                                               workers=workers)\n",
    "    \n",
    "    classifier.fit(X_train_dask, y_train_dask)\n",
    "    y_pred = classifier.predict(X_valid_dask)\n",
    "    score = accuracy_score(y_valid, y_pred.compute())\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 02:22:40,812] A new study created with name: Multi-GPU\n",
      "[I 2020-07-02 02:22:49,475] Finished trial#0 with value: 0.8308113813400269 with parameters: {'max_depth': 11, 'n_estimators': 389}. Best is trial#0 with value: 0.8308113813400269.\n",
      "[I 2020-07-02 02:23:57,777] Finished trial#1 with value: 0.8306707739830017 with parameters: {'max_depth': 15, 'n_estimators': 644}. Best is trial#0 with value: 0.8308113813400269.\n",
      "[I 2020-07-02 02:24:20,013] Finished trial#2 with value: 0.8308956027030945 with parameters: {'max_depth': 15, 'n_estimators': 218}. Best is trial#2 with value: 0.8308956027030945.\n",
      "[I 2020-07-02 02:24:53,807] Finished trial#3 with value: 0.8309198021888733 with parameters: {'max_depth': 14, 'n_estimators': 689}. Best is trial#3 with value: 0.8309198021888733.\n",
      "[I 2020-07-02 02:25:14,744] Finished trial#4 with value: 0.8307151794433594 with parameters: {'max_depth': 15, 'n_estimators': 209}. Best is trial#3 with value: 0.8309198021888733.\n",
      "[I 2020-07-02 02:25:30,107] Finished trial#5 with value: 0.830748975276947 with parameters: {'max_depth': 11, 'n_estimators': 739}. Best is trial#3 with value: 0.8309198021888733.\n",
      "[I 2020-07-02 02:25:49,615] Finished trial#6 with value: 0.8305298089981079 with parameters: {'max_depth': 12, 'n_estimators': 704}. Best is trial#3 with value: 0.8309198021888733.\n",
      "[I 2020-07-02 02:25:57,340] Finished trial#7 with value: 0.8306301832199097 with parameters: {'max_depth': 10, 'n_estimators': 417}. Best is trial#3 with value: 0.8309198021888733.\n",
      "[I 2020-07-02 02:26:27,193] Finished trial#8 with value: 0.8306502103805542 with parameters: {'max_depth': 14, 'n_estimators': 613}. Best is trial#3 with value: 0.8309198021888733.\n",
      "[I 2020-07-02 02:26:57,057] Finished trial#9 with value: 0.8306450247764587 with parameters: {'max_depth': 15, 'n_estimators': 292}. Best is trial#3 with value: 0.8309198021888733.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..multi-GPU-estimators    :  256.6885\n"
     ]
    }
   ],
   "source": [
    "with timed(\"multi-GPU-estimators\"):\n",
    "    with performance_report(filename=\"dask-report-mnmg.html\"):\n",
    "        study = optuna.create_study(sampler= optuna.samplers.TPESampler(),\n",
    "                                    study_name=\"Multi-GPU\",\n",
    "                                    direction=\"maximize\",\n",
    "                                    storage=\"sqlite:///mnmg.db\")\n",
    "        study.optimize(objective_mg, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the timing results\n",
    "\n",
    "| Study name | Runtime |   \n",
    "|---|---|\n",
    "| Optuna-Multi-GPU-TPE | 91.3055 |\n",
    "| Optuna-Multi-GPU-CMAE | 88.3086 |\n",
    "| No-Optuna-Call | 89.2947 |\n",
    "| Optuna-MLflow-callback | 88.1154 |\n",
    "| Multi-GPU-Estimator | 35.9608 |\n",
    "\n",
    "We noteice that with 2 GPUS, we were able to run the multi-GPU estimator more than twice as fast as the other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CPU with 750 estimators max does not finish running after hours.\n",
    "# def objective_cpu(trial):\n",
    "    \n",
    "#     max_depth = trial.suggest_int(\"max_depth\", 5, 15)\n",
    "#     n_estimators = trial.suggest_int(\"n_estimators\", 100, 750)\n",
    "\n",
    "#     classifier = sklearn.ensemble.RandomForestRegressor(max_depth=max_depth,\n",
    "#                                        n_estimators=n_estimators)\n",
    "\n",
    "#     X_train, X_valid, y_train, y_valid = sklearn.model_selection.train_test_split(X_, y_)\n",
    "    \n",
    "#     classifier.fit(X_train, y_train)\n",
    "#     y_pred = classifier.predict(X_valid)\n",
    "    \n",
    "#     score = accuracy_score(y_valid, y_pred)\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with timed(\"cpu-etl\"):\n",
    "#     df_pd = pd.read_parquet(INPUT_FILE)\n",
    "#     X_, y_ = df_pd.drop([\"ArrDelayBinary\"], axis=1), df_pd[\"ArrDelayBinary\"].astype('int32')\n",
    "    \n",
    "# with timed(\"cpu-hpo\"):\n",
    "#     study = optuna.create_study(direction=\"maximize\") # Equivalent to an experiment, a set of trials\n",
    "#     study.optimize(objective_cpu, n_trials=N_TRIALS, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_higgs(compressed_filepath, decompressed_filepath):\n",
    "#     higgs_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz'\n",
    "#     if not os.path.isfile(compressed_filepath):\n",
    "#         urlretrieve(higgs_url, compressed_filepath)\n",
    "#     if not os.path.isfile(decompressed_filepath):\n",
    "#         cf = gzip.GzipFile(compressed_filepath)\n",
    "#         with open(decompressed_filepath, 'wb') as df:\n",
    "#             df.write(cf.read())\n",
    "# import os\n",
    "# from urllib.request import urlretrieve\n",
    "# import gzip\n",
    "\n",
    "# data_dir = '/home/hyperopt/data/'\n",
    "# if not os.path.exists(data_dir):\n",
    "#     print('creating data directory')\n",
    "#     os.system('mkdir /home/data/')\n",
    "    \n",
    "# compressed_filepath = os.path.join(data_dir, 'HIGGS.csv.gz') # Set this as path for gzipped Higgs data file, if you already have\n",
    "# decompressed_filepath = os.path.join(data_dir, 'HIGGS.csv') # Set this as path for decompressed Higgs data file, if you already have\n",
    "\n",
    "# # Uncomment this line to download the dataset.\n",
    "# # download_higgs(compressed_filepath, decompressed_filepath)\n",
    "\n",
    "# col_names = ['label'] + [\"col-{}\".format(i) for i in range(2, 30)] # Assign column names\n",
    "# dtypes_ls = ['int32'] + ['float32' for _ in range(2, 30)] # Assign dtypes to each column\n",
    "# input_data = cudf.read_csv(decompressed_filepath, names=col_names, dtype=dtypes_ls)\n",
    "\n",
    "# labels = input_data.label.reset_index().drop(['index'], axis=1)\n",
    "# for col in labels.columns:\n",
    "#     labels[col] = labels[col].astype('float32')\n",
    "# data = input_data.drop(['label'], axis=1)\n",
    "\n",
    "# N_ROWS = int(len(data) * data_fraction)\n",
    "\n",
    "# X = data[:N_ROWS]\n",
    "# y = labels[:N_ROWS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
