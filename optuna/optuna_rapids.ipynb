{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Optuna + RAPIDS\n",
    "\n",
    "Optuna is a lightweight framework for hyperparameter optimization. It provides a code-by-run method which makes it easy to adapt to any already existing code that we have. Just wrapping the objective function with Optuna can help perform a parallel-distributed HPO search over a search space.\n",
    "\n",
    "We'll explore how to use Optuna with RAPIDS and run multi-GPU HPO runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes - \n",
    "\n",
    "1. Using default SVM parameters (\"Rbf\" kernel) with full airline data results in `cudaErrorMemoryAllocation` - out of memory when `fit` is called.\n",
    "2. Even with 1/10th the data linear kernel hangs for a long time (is this a possible bug or expected behavior?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/treelite/gallery/__init__.py:7: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/treelite/gallery/sklearn/__init__.py:9: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cudf\n",
    "import cuml\n",
    "import dask_cudf\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.metrics import accuracy_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from dask.distributed import Client, wait, performance_report\n",
    "from joblib import parallel_backend, Parallel, delayed\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for timing blocks of code.\n",
    "@contextmanager\n",
    "def timed(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(\"..%-24s:  %8.4f\" % (name, t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.17.0.2:46683</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.17.0.2:8002/status' target='_blank'>http://172.17.0.2:8002/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>49.16 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.17.0.2:46683' processes=2 threads=2, memory=49.16 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(threads_per_worker=1, ip=\"\", dashboard_address=\"8002\")\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_streams = 8 # Performance optimization\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll load the airline data from the path specified by `INPUT_FILE`. The aim of the problem is to predict whether a plane will be delayed or not by the target variable `ArrDelayBinary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 10\n",
    "\n",
    "INPUT_FILE = \"/home/hyperopt/hyperopt/data/air_par.parquet\"\n",
    "df = cudf.read_parquet(INPUT_FILE)\n",
    "X, y = df.drop([\"ArrDelayBinary\"], axis=1), df[\"ArrDelayBinary\"].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "Here, we define `train_and_eval` function which simply fits a RandomForestClassifier (with`max_depth` and `n_estimators`) on the passed `X_param`, `y_param`. This function should look very similar for any ML workflow. We'll use this function within the Optuna `objective` function to show how easily we can fit an existing workflow into the Optuna work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(X_param, y_param, max_depth=16, n_estimators=100):\n",
    "    \"\"\"\n",
    "        Splits the given data into train and test split to train and evaluate the model\n",
    "        for the params parameters.\n",
    "        \n",
    "        Params\n",
    "        ______\n",
    "        \n",
    "        X_param:  DataFrame. \n",
    "                  The data to use for training and testing. \n",
    "        y_param:  Series. \n",
    "                  The label for training\n",
    "        max_depth, n_estimators: The values to use for max_depth and n_estimators for RFC.\n",
    "                                 Defaults to 16 and 100 (the defaults for the classifiers used)\n",
    "                   \n",
    "        Returns\n",
    "        score: Accuracy score of the fitted model\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_param, y_param, random_state=77)\n",
    "    classifier = cuml.ensemble.RandomForestClassifier(max_depth=max_depth,\n",
    "                     n_estimators=n_estimators)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_valid)\n",
    "    score = accuracy_score(y_valid, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a baseline number, let's see what the default performance of RFC is. Note the defauly values for `max_depth` = 16 and `n_estimators` = 100; we pass these to the `train_and_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with default parameters :  0.8379114270210266\n"
     ]
    }
   ],
   "source": [
    "print(\"Score with default parameters : \",train_and_eval(X, y, max_depth=16, n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "The objective function will be the one we optimize in Optuna studys. Objective funciton tries out different values for the parameters that we are tuning and saving the results in `study.trials_dataframes()`. \n",
    "\n",
    "Let's define the objective function for this HPO task by making use of the `train_and_eval()`. You can see that we simply choose a value for the parameters and call the `train_and_eval` method, making Optuna very easy to use in an existing workflow.\n",
    "\n",
    "The objective remains constant over different samplers, which are built-in options in Optuna to enable the selection of different sampling algorithms that optuna provides. Some of the available ones include - GridSampler, RandomSampler, TPESampler, etc. We'll try out different samplers and compare their performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_param, y_param):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 15)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 200, 700)\n",
    "    score = train_and_eval(X_param, y_param, max_depth=max_depth,\n",
    "                           n_estimators=n_estimators)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Trials and Study\n",
    "\n",
    "Optuna uses [study](https://optuna.readthedocs.io/en/stable/reference/study.html) and [trials](https://optuna.readthedocs.io/en/stable/reference/trial.html) to keep track of the HPO experiments. \n",
    "\n",
    "We'll make use of a helper function `run_study` to help us run one multi-GPU study with a dask backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_study(sampler=optuna.samplers.TPESampler(),\n",
    "              study_name=\"Optuna-MultiGPU\",\n",
    "              callbacks=None):\n",
    "    \n",
    "    with timed(study_name):\n",
    "        study = optuna.create_study(sampler=sampler,\n",
    "                                    study_name=study_name,\n",
    "                                    storage=\"sqlite:///_\"+study_name+\".db\",\n",
    "                                    direction=\"maximize\",\n",
    "                                    load_if_exists=True)\n",
    "        \n",
    "        with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "            study.optimize(lambda trial: objective(trial, X, y),\n",
    "                           n_trials=N_TRIALS,\n",
    "                           n_jobs=n_workers,\n",
    "                           callbacks=callbacks)\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: \", trial.value)\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:17:55,658] A new study created with name: optuna-joblib-dask-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-joblib-dask-backend:  219.5348\n",
      "Number of finished trials:  10\n",
      "Best trial:\n",
      "  Value:  0.8429234027862549\n",
      "  Params: \n",
      "    max_depth: 15\n",
      "    n_estimators: 662\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-joblib-dask-backend\"\n",
    "with performance_report(filename=name+\"-dask_report.html\"):\n",
    "    study_tpe = run_study(optuna.samplers.TPESampler(),\n",
    "                          study_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:21:36,133] A new study created with name: optuna-joblib-loky-backend\n",
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-joblib-loky-backend:  333.9446\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-joblib-loky-backend\"\n",
    "\n",
    "with timed(name):\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                                study_name=name,\n",
    "                                storage=\"sqlite:///_\"+name+\".db\",\n",
    "                                direction=\"maximize\",\n",
    "                                load_if_exists=True)\n",
    "    with parallel_backend(\"loky\", n_jobs=n_workers):\n",
    "        study.optimize(lambda trial: objective(trial, X, y),\n",
    "                       n_trials=N_TRIALS,\n",
    "                       n_jobs=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:27:10,102] A new study created with name: optuna-simple\n",
      "[I 2020-07-02 19:27:39,800] Finished trial#0 with value: 0.830892026424408 with parameters: {'max_depth': 12, 'n_estimators': 478}. Best is trial#0 with value: 0.830892026424408.\n",
      "[I 2020-07-02 19:28:05,566] Finished trial#1 with value: 0.8308805823326111 with parameters: {'max_depth': 12, 'n_estimators': 418}. Best is trial#0 with value: 0.830892026424408.\n",
      "[I 2020-07-02 19:28:22,502] Finished trial#2 with value: 0.8308159708976746 with parameters: {'max_depth': 14, 'n_estimators': 213}. Best is trial#0 with value: 0.830892026424408.\n",
      "[I 2020-07-02 19:28:41,470] Finished trial#3 with value: 0.8308290243148804 with parameters: {'max_depth': 12, 'n_estimators': 308}. Best is trial#0 with value: 0.830892026424408.\n",
      "[I 2020-07-02 19:29:18,875] Finished trial#4 with value: 0.8310192227363586 with parameters: {'max_depth': 11, 'n_estimators': 694}. Best is trial#4 with value: 0.8310192227363586.\n",
      "[I 2020-07-02 19:29:47,318] Finished trial#5 with value: 0.8307967782020569 with parameters: {'max_depth': 14, 'n_estimators': 354}. Best is trial#4 with value: 0.8310192227363586.\n",
      "[I 2020-07-02 19:30:04,932] Finished trial#6 with value: 0.8340274095535278 with parameters: {'max_depth': 10, 'n_estimators': 376}. Best is trial#6 with value: 0.8340274095535278.\n",
      "[I 2020-07-02 19:30:22,885] Finished trial#7 with value: 0.8307582139968872 with parameters: {'max_depth': 13, 'n_estimators': 259}. Best is trial#6 with value: 0.8340274095535278.\n",
      "[I 2020-07-02 19:31:18,603] Finished trial#8 with value: 0.830810010433197 with parameters: {'max_depth': 14, 'n_estimators': 683}. Best is trial#6 with value: 0.8340274095535278.\n",
      "[I 2020-07-02 19:31:45,576] Finished trial#9 with value: 0.8310533761978149 with parameters: {'max_depth': 11, 'n_estimators': 502}. Best is trial#6 with value: 0.8340274095535278.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..no-dask-no-joblib       :  275.6379\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-simple\"\n",
    "with timed(\"no-dask-no-joblib\"):\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                                study_name=name,\n",
    "                                storage=\"sqlite:///_\"+name+\".db\",\n",
    "                                direction=\"maximize\",\n",
    "                                load_if_exists=True)\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential calls without Optuna\n",
    "\n",
    "For a comparison let's try sequential calls without Optuna and it's parallel-processing support. We can cleared see that it takes more time to do this. We'll pick the same parameters as Optuna for a fair comparison - these parameters were selected by the sampling algorithm used by Optuna and is available in the `study.trials_dataframe()` for us to pick out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study_tpe.trials_dataframe()\n",
    "params_max_depth, params_n_estimators = df['params_max_depth'], df['params_n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential call function \n",
    "\n",
    "For a cleaner look, let's use a function to perform sequential calls. The function basically sets the parameters to what was passed and trains and evaluates the model and returns the details of the run which can later be used to find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_call(X, y, max_depth, n_estimators):\n",
    "    \n",
    "    score = train_and_eval(X, y, max_depth=max_depth, n_estimators = n_estimators)\n",
    "    \n",
    "    return score, max_depth, n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.830841600894928, 12, 460), (0.8307613730430603, 13, 567), (0.8310205936431885, 11, 531), (0.8307623863220215, 13, 362), (0.8307573795318604, 13, 414), (0.8429433703422546, 15, 662), (0.8341590166091919, 10, 588), (0.8307747840881348, 13, 325), (0.8308181762695312, 14, 391), (0.831036388874054, 11, 426)]\n",
      "..joblib-dask-backend     :  177.4734\n"
     ]
    }
   ],
   "source": [
    "name = \"joblib-dask-backend\"\n",
    "with timed(name):\n",
    "    with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "        results = Parallel()(delayed(seq_call)(X, y, max_depth=params_max_depth[i],\n",
    "                     n_estimators=params_n_estimators[i]) for i in range(N_TRIALS))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Running this without a dask backend is actually faster - takes about 65 seconds to finish by just making N_TRIALS sequential calls. Dask backend makes most sense when used with multi-GPU estimators as we see later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8310825824737549, 11, 426)\n",
      "..sequential-calls        :  339.9721\n"
     ]
    }
   ],
   "source": [
    "name = \"sequential-calls\"\n",
    "with timed(name):\n",
    "    for i in range(N_TRIALS):\n",
    "        results = seq_call(X, y, max_depth=params_max_depth[i],\n",
    "                     n_estimators=params_n_estimators[i])\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow callback\n",
    "\n",
    "Optuna supports the integration of various libraries. One of them is a tracking library MLflow, this is used to keep track of the different Hyperopt runs. We can simply add it by adding a callback to a study as shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_callback(study, trial):\n",
    "    trial_value = trial.value if trial.value is not None else float(\"nan\")\n",
    "    with mlflow.start_run(run_name=study.study_name):\n",
    "        print(trial.params)\n",
    "#         mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "        mlflow.log_params(trial.params)\n",
    "        mlflow.log_metrics({\"accuracy\": trial_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:40:23,263] A new study created with name: optuna-joblib-dask-backend-mlflow-callback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-joblib-dask-backend-mlflow-callback:  199.5456\n",
      "Number of finished trials:  10\n",
      "Best trial:\n",
      "  Value:  0.8341314196586609\n",
      "  Params: \n",
      "    max_depth: 10\n",
      "    n_estimators: 464\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-joblib-dask-backend-mlflow-callback\"\n",
    "study = run_study(optuna.samplers.TPESampler(),\n",
    "                  study_name=name,\n",
    "                  callbacks=[mlflow_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU estimators\n",
    "\n",
    "We also have estimators that can run on multiple GPUs. `cuml.dask` has a set of multi-GPU estimators that can run incredibly fast. Let's try that out. In order to do this, we need to used `dask_cudf` dataframes and we will redefine the objective function from earlier to do just that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mg(trial, X, y):\n",
    "#     return 1\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 15)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 200, 700)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=77)\n",
    "    \n",
    "    # Multi-GPU Estimator\n",
    "    from cuml.dask.ensemble import RandomForestClassifier\n",
    "    classifier = cuml.dask.ensemble.RandomForestClassifier(max_depth=max_depth,\n",
    "                         n_estimators=n_estimators)\n",
    "\n",
    "    # Necessary conversions for cuml.dask.ensemble\n",
    "    X_train_dask = dask_cudf.from_cudf(X_train, npartitions=2)\n",
    "    X_valid_dask = dask_cudf.from_cudf(X_valid, npartitions=2)\n",
    "\n",
    "    y_train_dask = dask_cudf.from_cudf(y_train, npartitions=2)\n",
    "    y_valid_dask = dask_cudf.from_cudf(y_valid, npartitions=2)\n",
    "\n",
    "    X_train_dask, X_valid_dask, \\\n",
    "    y_train_dask, y_valid_dask = dask_utils.persist_across_workers(c,[X_train_dask,\n",
    "                                                                      X_valid_dask,\n",
    "                                                                      y_train_dask,\n",
    "                                                                      y_valid_dask],\n",
    "                                                                      workers=workers)\n",
    "\n",
    "    classifier.fit(X_train_dask, y_train_dask)\n",
    "    y_pred = classifier.predict(X_valid_dask)\n",
    "    score = accuracy_score(y_valid, y_pred.compute())\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 20:37:41,322] A new study created with name: optuna-mnmg-joblib\n",
      "distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:38:10,189] Finished trial#0 with value: 0.8307306170463562 with parameters: {'max_depth': 13, 'n_estimators': 652}. Best is trial#0 with value: 0.8307306170463562.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:38:32,745] Finished trial#1 with value: 0.8307396173477173 with parameters: {'max_depth': 14, 'n_estimators': 468}. Best is trial#1 with value: 0.8307396173477173.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:38:49,159] Finished trial#2 with value: 0.830734372138977 with parameters: {'max_depth': 12, 'n_estimators': 596}. Best is trial#1 with value: 0.8307396173477173.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:38:56,884] Finished trial#3 with value: 0.830734372138977 with parameters: {'max_depth': 12, 'n_estimators': 280}. Best is trial#1 with value: 0.8307396173477173.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:39:08,417] Finished trial#4 with value: 0.8307392001152039 with parameters: {'max_depth': 14, 'n_estimators': 238}. Best is trial#1 with value: 0.8307396173477173.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:39:15,259] Finished trial#5 with value: 0.8307433724403381 with parameters: {'max_depth': 11, 'n_estimators': 294}. Best is trial#5 with value: 0.8307433724403381.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:39:22,923] Finished trial#6 with value: 0.8307284116744995 with parameters: {'max_depth': 11, 'n_estimators': 343}. Best is trial#5 with value: 0.8307433724403381.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 28% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:39:52,718] Finished trial#7 with value: 0.8307297825813293 with parameters: {'max_depth': 15, 'n_estimators': 297}. Best is trial#5 with value: 0.8307433724403381.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:39:59,622] Finished trial#8 with value: 0.8307533860206604 with parameters: {'max_depth': 12, 'n_estimators': 242}. Best is trial#8 with value: 0.8307533860206604.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "[I 2020-07-02 20:40:11,760] Finished trial#9 with value: 0.8307381868362427 with parameters: {'max_depth': 11, 'n_estimators': 575}. Best is trial#8 with value: 0.8307533860206604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-mnmg-joblib      :  150.6013\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-mnmg-joblib\"\n",
    "with performance_report(filename=name+\".html\"):\n",
    "    with timed(name):\n",
    "        study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                                    study_name=name,\n",
    "                                    storage=\"sqlite:///_\"+name+\".db\",\n",
    "                                    direction=\"maximize\",\n",
    "                                    load_if_exists=True)\n",
    "        with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "            study.optimize(lambda trial: objective_mg(trial, X, y),\n",
    "                               n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the timing results\n",
    "\n",
    "| Study name | Runtime |   \n",
    "|---|---|\n",
    "| Optuna-Multi-GPU-TPE | 267.5325 |\n",
    "| Loky-Backend | 270.1898 |\n",
    "| No-dask-No-Joblib | 289.1285 |\n",
    "| Dask-no-Optuna | 175.0705 |\n",
    "| No-Optuna-No-dask-Seq-Call | 315.9573 |\n",
    "| Multi-GPU-Estimator | 218.8659 |\n",
    "\n",
    "We noteice that with 2 GPUS, we were able to run the multi-GPU estimator more than twice as fast as the other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
