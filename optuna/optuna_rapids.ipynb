{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Optuna + RAPIDS\n",
    "\n",
    "Optuna is a lightweight framework for hyperparameter optimization. It provides a code-by-run method which makes it easy to adapt to any already existing code that we have. Just wrapping the objective function with Optuna can help perform a parallel-distributed HPO search over a search space.\n",
    "\n",
    "We'll explore how to use Optuna with RAPIDS and run multi-GPU HPO runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes - \n",
    "\n",
    "1. Using default SVM parameters (\"Rbf\" kernel) with full airline data results in `cudaErrorMemoryAllocation` - out of memory when `fit` is called.\n",
    "2. Even with 1/10th the data linear kernel hangs for a long time (is this a possible bug or expected behavior?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/treelite/gallery/__init__.py:7: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/treelite/gallery/sklearn/__init__.py:9: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cudf\n",
    "import cuml\n",
    "import dask_cudf\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.metrics import accuracy_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from dask.distributed import Client, wait, performance_report\n",
    "from joblib import parallel_backend, Parallel, delayed\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for timing blocks of code.\n",
    "@contextmanager\n",
    "def timed(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(\"..%-24s:  %8.4f\" % (name, t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.17.0.2:37667</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.17.0.2:8002/status' target='_blank'>http://172.17.0.2:8002/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>49.16 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.17.0.2:37667' processes=2 threads=2, memory=49.16 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(threads_per_worker=1, ip=\"\", dashboard_address=\"8002\")\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_streams = 8 # Performance optimization\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll load the airline data from the path specified by `INPUT_FILE`. The aim of the problem is to predict whether a plane will be delayed or not by the target variable `ArrDelayBinary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 20\n",
    "\n",
    "INPUT_FILE = \"/home/hyperopt/hyperopt/data/air_par.parquet\"\n",
    "df = cudf.read_parquet(INPUT_FILE)\n",
    "X, y = df.drop([\"ArrDelayBinary\"], axis=1), df[\"ArrDelayBinary\"].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "Here, we define `train_and_eval` function which simply fits a RandomForestClassifier (with`max_depth` and `n_estimators`) on the passed `X_param`, `y_param`. This function should look very similar for any ML workflow. We'll use this function within the Optuna `objective` function to show how easily we can fit an existing workflow into the Optuna work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(X_param, y_param, max_depth=16, n_estimators=100):\n",
    "    \"\"\"\n",
    "        Splits the given data into train and test split to train and evaluate the model\n",
    "        for the params parameters.\n",
    "        \n",
    "        Params\n",
    "        ______\n",
    "        \n",
    "        X_param:  DataFrame. \n",
    "                  The data to use for training and testing. \n",
    "        y_param:  Series. \n",
    "                  The label for training\n",
    "        max_depth, n_estimators: The values to use for max_depth and n_estimators for RFC.\n",
    "                                 Defaults to 16 and 100 (the defaults for the classifiers used)\n",
    "                   \n",
    "        Returns\n",
    "        score: Accuracy score of the fitted model\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_param, y_param, random_state=77)\n",
    "    classifier = cuml.ensemble.RandomForestClassifier(max_depth=max_depth,\n",
    "                     n_estimators=n_estimators)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_valid)\n",
    "    score = accuracy_score(y_valid, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a baseline number, let's see what the default performance of RFC is. Note the defauly values for `max_depth` = 16 and `n_estimators` = 100; we pass these to the `train_and_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with default parameters :  0.8378934264183044\n"
     ]
    }
   ],
   "source": [
    "print(\"Score with default parameters : \",train_and_eval(X, y, max_depth=16, n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "The objective function will be the one we optimize in Optuna studys. Objective funciton tries out different values for the parameters that we are tuning and saving the results in `study.trials_dataframes()`. \n",
    "\n",
    "Let's define the objective function for this HPO task by making use of the `train_and_eval()`. You can see that we simply choose a value for the parameters and call the `train_and_eval` method, making Optuna very easy to use in an existing workflow.\n",
    "\n",
    "The objective remains constant over different samplers, which are built-in options in Optuna to enable the selection of different sampling algorithms that optuna provides. Some of the available ones include - GridSampler, RandomSampler, TPESampler, etc. We'll try out different samplers and compare their performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_param, y_param):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 15)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 200, 700)\n",
    "    score = train_and_eval(X_param, y_param, max_depth=max_depth,\n",
    "                           n_estimators=n_estimators)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Trials and Study\n",
    "\n",
    "Optuna uses [study](https://optuna.readthedocs.io/en/stable/reference/study.html) and [trials](https://optuna.readthedocs.io/en/stable/reference/trial.html) to keep track of the HPO experiments. \n",
    "\n",
    "We'll make use of a helper function `run_study` to help us run one multi-GPU study with a dask backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_study(sampler=optuna.samplers.TPESampler(),\n",
    "              study_name=\"Optuna-MultiGPU\",\n",
    "              callbacks=None):\n",
    "    \n",
    "    with timed(study_name):\n",
    "        study = optuna.create_study(sampler=sampler,\n",
    "                                    study_name=study_name,\n",
    "                                    storage=\"sqlite:///_\"+study_name+\".db\",\n",
    "                                    direction=\"maximize\",\n",
    "                                    load_if_exists=True)\n",
    "        \n",
    "        with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "            study.optimize(lambda trial: objective(trial, X, y),\n",
    "                           n_trials=N_TRIALS,\n",
    "                           n_jobs=n_workers,\n",
    "                           callbacks=callbacks)\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: \", trial.value)\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 22:10:48,638] A new study created with name: optuna-joblib-dask-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-joblib-dask-backend:  565.8170\n",
      "Number of finished trials:  20\n",
      "Best trial:\n",
      "  Value:  0.8430125713348389\n",
      "  Params: \n",
      "    max_depth: 15\n",
      "    n_estimators: 559\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-joblib-dask-backend\"\n",
    "with performance_report(filename=name+\"-dask_report.html\"):\n",
    "    study_tpe = run_study(optuna.samplers.TPESampler(),\n",
    "                          study_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 22:20:15,451] A new study created with name: optuna-joblib-loky-backend\n",
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-joblib-loky-backend:  672.4247\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-joblib-loky-backend\"\n",
    "\n",
    "with timed(name):\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                                study_name=name,\n",
    "                                storage=\"sqlite:///_\"+name+\".db\",\n",
    "                                direction=\"maximize\",\n",
    "                                load_if_exists=True)\n",
    "    with parallel_backend(\"loky\", n_jobs=n_workers):\n",
    "        study.optimize(lambda trial: objective(trial, X, y),\n",
    "                       n_trials=N_TRIALS,\n",
    "                       n_jobs=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 22:31:27,931] A new study created with name: optuna-simple\n",
      "[I 2020-07-02 22:32:30,525] Finished trial#0 with value: 0.842966616153717 with parameters: {'max_depth': 15, 'n_estimators': 499}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:32:45,610] Finished trial#1 with value: 0.8308876156806946 with parameters: {'max_depth': 12, 'n_estimators': 245}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:33:19,147] Finished trial#2 with value: 0.8428270220756531 with parameters: {'max_depth': 15, 'n_estimators': 276}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:33:49,459] Finished trial#3 with value: 0.8307697772979736 with parameters: {'max_depth': 13, 'n_estimators': 435}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:34:00,572] Finished trial#4 with value: 0.8310322165489197 with parameters: {'max_depth': 11, 'n_estimators': 200}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:34:33,019] Finished trial#5 with value: 0.8428840041160583 with parameters: {'max_depth': 15, 'n_estimators': 267}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:34:54,019] Finished trial#6 with value: 0.8341163992881775 with parameters: {'max_depth': 10, 'n_estimators': 452}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:35:06,929] Finished trial#7 with value: 0.8309332132339478 with parameters: {'max_depth': 11, 'n_estimators': 235}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:35:33,805] Finished trial#8 with value: 0.8307690024375916 with parameters: {'max_depth': 13, 'n_estimators': 385}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:36:01,489] Finished trial#9 with value: 0.8342682123184204 with parameters: {'max_depth': 10, 'n_estimators': 597}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:36:55,151] Finished trial#10 with value: 0.8308050036430359 with parameters: {'max_depth': 14, 'n_estimators': 667}. Best is trial#0 with value: 0.842966616153717.\n",
      "[I 2020-07-02 22:38:05,292] Finished trial#11 with value: 0.8430172204971313 with parameters: {'max_depth': 15, 'n_estimators': 562}. Best is trial#11 with value: 0.8430172204971313.\n",
      "[I 2020-07-02 22:39:13,767] Finished trial#12 with value: 0.8429775834083557 with parameters: {'max_depth': 15, 'n_estimators': 554}. Best is trial#11 with value: 0.8430172204971313.\n",
      "[I 2020-07-02 22:40:00,034] Finished trial#13 with value: 0.8307926058769226 with parameters: {'max_depth': 14, 'n_estimators': 580}. Best is trial#11 with value: 0.8430172204971313.\n",
      "[I 2020-07-02 22:40:55,208] Finished trial#14 with value: 0.8308122158050537 with parameters: {'max_depth': 14, 'n_estimators': 688}. Best is trial#11 with value: 0.8430172204971313.\n",
      "[I 2020-07-02 22:42:05,545] Finished trial#15 with value: 0.8430259823799133 with parameters: {'max_depth': 15, 'n_estimators': 565}. Best is trial#15 with value: 0.8430259823799133.\n",
      "[I 2020-07-02 22:42:55,509] Finished trial#16 with value: 0.8307837843894958 with parameters: {'max_depth': 14, 'n_estimators': 624}. Best is trial#15 with value: 0.8430259823799133.\n",
      "[I 2020-07-02 22:43:32,266] Finished trial#17 with value: 0.8307626247406006 with parameters: {'max_depth': 13, 'n_estimators': 524}. Best is trial#15 with value: 0.8430259823799133.\n",
      "[I 2020-07-02 22:44:52,543] Finished trial#18 with value: 0.8429853916168213 with parameters: {'max_depth': 15, 'n_estimators': 640}. Best is trial#15 with value: 0.8430259823799133.\n",
      "[I 2020-07-02 22:45:21,847] Finished trial#19 with value: 0.830807626247406 with parameters: {'max_depth': 14, 'n_estimators': 371}. Best is trial#15 with value: 0.8430259823799133.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..no-dask-no-joblib       :  834.1236\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-simple\"\n",
    "with timed(\"no-dask-no-joblib\"):\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                                study_name=name,\n",
    "                                storage=\"sqlite:///_\"+name+\".db\",\n",
    "                                direction=\"maximize\",\n",
    "                                load_if_exists=True)\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential calls without Optuna\n",
    "\n",
    "For a comparison let's try sequential calls without Optuna and it's parallel-processing support. We can cleared see that it takes more time to do this. We'll pick the same parameters as Optuna for a fair comparison - these parameters were selected by the sampling algorithm used by Optuna and is available in the `study.trials_dataframe()` for us to pick out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study_tpe.trials_dataframe()\n",
    "params_max_depth, params_n_estimators = df['params_max_depth'], df['params_n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential call function \n",
    "\n",
    "For a cleaner look, let's use a function to perform sequential calls. The function basically sets the parameters to what was passed and trains and evaluates the model and returns the details of the run which can later be used to find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_call(X, y, max_depth, n_estimators):\n",
    "    \n",
    "    score = train_and_eval(X, y, max_depth=max_depth, n_estimators = n_estimators)\n",
    "    \n",
    "    return score, max_depth, n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8343474268913269, 10, 669), (0.8308039903640747, 14, 342), (0.830763578414917, 13, 514), (0.8310037851333618, 11, 675), (0.8310263752937317, 11, 400), (0.8309957981109619, 11, 258), (0.8310484290122986, 11, 357), (0.8340405821800232, 10, 349), (0.842937171459198, 15, 608), (0.8310012221336365, 11, 355), (0.8430197834968567, 15, 686), (0.842921793460846, 15, 574), (0.8429651856422424, 15, 571), (0.8429498076438904, 15, 559), (0.8307988047599792, 14, 582), (0.8307899832725525, 14, 491), (0.8307834267616272, 14, 488), (0.8307684063911438, 13, 514), (0.8307774066925049, 13, 627), (0.8429650068283081, 15, 631)]\n",
      "..joblib-dask-backend     :  478.0961\n"
     ]
    }
   ],
   "source": [
    "name = \"joblib-dask-backend\"\n",
    "with timed(name):\n",
    "    with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "        results = Parallel()(delayed(seq_call)(X, y, max_depth=params_max_depth[i],\n",
    "                     n_estimators=params_n_estimators[i]) for i in range(N_TRIALS))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Running this without a dask backend is actually faster - takes about 65 seconds to finish by just making N_TRIALS sequential calls. Dask backend makes most sense when used with multi-GPU estimators as we see later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8429834246635437, 15, 631)\n",
      "..sequential-calls        :  873.5314\n"
     ]
    }
   ],
   "source": [
    "name = \"sequential-calls\"\n",
    "with timed(name):\n",
    "    for i in range(N_TRIALS):\n",
    "        results = seq_call(X, y, max_depth=params_max_depth[i],\n",
    "                     n_estimators=params_n_estimators[i])\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow callback\n",
    "\n",
    "Optuna supports the integration of various libraries. One of them is a tracking library MLflow, this is used to keep track of the different Hyperopt runs. We can simply add it by adding a callback to a study as shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_callback(study, trial):\n",
    "    trial_value = trial.value if trial.value is not None else float(\"nan\")\n",
    "    with mlflow.start_run(run_name=study.study_name):\n",
    "        print(trial.params)\n",
    "#         mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "        mlflow.log_params(trial.params)\n",
    "        mlflow.log_metrics({\"accuracy\": trial_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 23:07:53,706] A new study created with name: optuna-joblib-dask-backend-mlflow-callback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-joblib-dask-backend-mlflow-callback:  566.7905\n",
      "Number of finished trials:  20\n",
      "Best trial:\n",
      "  Value:  0.8430274128913879\n",
      "  Params: \n",
      "    max_depth: 15\n",
      "    n_estimators: 655\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-joblib-dask-backend-mlflow-callback\"\n",
    "study = run_study(optuna.samplers.TPESampler(),\n",
    "                  study_name=name,\n",
    "                  callbacks=[mlflow_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU estimators\n",
    "\n",
    "We also have estimators that can run on multiple GPUs. `cuml.dask` has a set of multi-GPU estimators that can run incredibly fast. Let's try that out. In order to do this, we need to used `dask_cudf` dataframes and we will redefine the objective function from earlier to do just that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mg(trial, X, y):\n",
    "#     return 1\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 15)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 200, 700)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=77)\n",
    "    \n",
    "    # Multi-GPU Estimator\n",
    "    from cuml.dask.ensemble import RandomForestClassifier\n",
    "    classifier = cuml.dask.ensemble.RandomForestClassifier(max_depth=max_depth,\n",
    "                         n_estimators=n_estimators)\n",
    "\n",
    "    # Necessary conversions for cuml.dask.ensemble\n",
    "    X_train_dask = dask_cudf.from_cudf(X_train, npartitions=2)\n",
    "    X_valid_dask = dask_cudf.from_cudf(X_valid, npartitions=2)\n",
    "\n",
    "    y_train_dask = dask_cudf.from_cudf(y_train, npartitions=2)\n",
    "    y_valid_dask = dask_cudf.from_cudf(y_valid, npartitions=2)\n",
    "\n",
    "    X_train_dask, X_valid_dask, \\\n",
    "    y_train_dask, y_valid_dask = dask_utils.persist_across_workers(c,[X_train_dask,\n",
    "                                                                      X_valid_dask,\n",
    "                                                                      y_train_dask,\n",
    "                                                                      y_valid_dask],\n",
    "                                                                      workers=workers)\n",
    "\n",
    "    classifier.fit(X_train_dask, y_train_dask)\n",
    "    y_pred = classifier.predict(X_valid_dask)\n",
    "    score = accuracy_score(y_valid, y_pred.compute())\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 23:17:20,675] A new study created with name: optuna-mnmg-joblib---\n",
      "[I 2020-07-02 23:18:06,496] Finished trial#0 with value: 0.8307358026504517 with parameters: {'max_depth': 15, 'n_estimators': 408}. Best is trial#0 with value: 0.8307358026504517.\n",
      "[I 2020-07-02 23:18:17,346] Finished trial#1 with value: 0.830734372138977 with parameters: {'max_depth': 10, 'n_estimators': 619}. Best is trial#0 with value: 0.8307358026504517.\n",
      "[I 2020-07-02 23:18:31,793] Finished trial#2 with value: 0.8307341933250427 with parameters: {'max_depth': 14, 'n_estimators': 307}. Best is trial#0 with value: 0.8307358026504517.\n",
      "[I 2020-07-02 23:18:45,384] Finished trial#3 with value: 0.8307167887687683 with parameters: {'max_depth': 14, 'n_estimators': 295}. Best is trial#0 with value: 0.8307358026504517.\n",
      "[I 2020-07-02 23:18:58,549] Finished trial#4 with value: 0.830733597278595 with parameters: {'max_depth': 11, 'n_estimators': 665}. Best is trial#0 with value: 0.8307358026504517.\n",
      "[I 2020-07-02 23:19:26,309] Finished trial#5 with value: 0.8307362198829651 with parameters: {'max_depth': 14, 'n_estimators': 594}. Best is trial#5 with value: 0.8307362198829651.\n",
      "[I 2020-07-02 23:19:34,604] Finished trial#6 with value: 0.8307440280914307 with parameters: {'max_depth': 10, 'n_estimators': 468}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:20:03,333] Finished trial#7 with value: 0.8307211995124817 with parameters: {'max_depth': 14, 'n_estimators': 609}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:20:13,290] Finished trial#8 with value: 0.830731213092804 with parameters: {'max_depth': 10, 'n_estimators': 591}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:20:48,802] Finished trial#9 with value: 0.8307424187660217 with parameters: {'max_depth': 15, 'n_estimators': 375}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:21:00,980] Finished trial#10 with value: 0.8307269811630249 with parameters: {'max_depth': 12, 'n_estimators': 485}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:21:11,757] Finished trial#11 with value: 0.8307337760925293 with parameters: {'max_depth': 12, 'n_estimators': 419}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:21:32,152] Finished trial#12 with value: 0.8307356238365173 with parameters: {'max_depth': 15, 'n_estimators': 215}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:21:42,022] Finished trial#13 with value: 0.8307284116744995 with parameters: {'max_depth': 11, 'n_estimators': 493}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:21:53,714] Finished trial#14 with value: 0.830731987953186 with parameters: {'max_depth': 13, 'n_estimators': 333}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:22:04,428] Finished trial#15 with value: 0.8307374119758606 with parameters: {'max_depth': 11, 'n_estimators': 527}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:22:17,211] Finished trial#16 with value: 0.8307257890701294 with parameters: {'max_depth': 13, 'n_estimators': 365}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:22:22,242] Finished trial#17 with value: 0.8307393789291382 with parameters: {'max_depth': 10, 'n_estimators': 219}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:23:02,949] Finished trial#18 with value: 0.8307219743728638 with parameters: {'max_depth': 15, 'n_estimators': 436}. Best is trial#6 with value: 0.8307440280914307.\n",
      "[I 2020-07-02 23:23:10,240] Finished trial#19 with value: 0.8307318091392517 with parameters: {'max_depth': 12, 'n_estimators': 265}. Best is trial#6 with value: 0.8307440280914307.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-mnmg-joblib---   :  349.7135\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-mnmg-joblib---\"\n",
    "with performance_report(filename=name+\".html\"):\n",
    "    with timed(name):\n",
    "        study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                                    study_name=name,\n",
    "                                    storage=\"sqlite:///_\"+name+\".db\",\n",
    "                                    direction=\"maximize\",\n",
    "                                    load_if_exists=True)\n",
    "        with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "            study.optimize(lambda trial: objective_mg(trial, X, y),\n",
    "                               n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the timing results\n",
    "\n",
    "| Study name | Runtime |   \n",
    "|---|---|\n",
    "| Optuna-Multi-GPU-TPE | 267.5325 |\n",
    "| Loky-Backend | 270.1898 |\n",
    "| No-dask-No-Joblib | 289.1285 |\n",
    "| Dask-no-Optuna | 175.0705 |\n",
    "| No-Optuna-No-dask-Seq-Call | 315.9573 |\n",
    "| Multi-GPU-Estimator | 218.8659 |\n",
    "\n",
    "We noteice that with 2 GPUS, we were able to run the multi-GPU estimator more than twice as fast as the other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
